{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fourmodern/toc_tutorial_colab/blob/main/teachopencadd/t110_esm2_peptide_optimization_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4668779",
      "metadata": {
        "id": "b4668779"
      },
      "source": [
        "\n",
        "# ESM-2 Protein-Peptide Binding Optimization Tutorial\n",
        "\n",
        "This notebook demonstrates how to use ESM-2, a protein language model from Facebook AI Research, to generate and optimize peptide binders for target proteins. The workflow includes:\n",
        "1. Setting up the environment\n",
        "2. Loading the ESM-2 model\n",
        "3. Preparing a protein sequence\n",
        "4. Generating peptide sequences\n",
        "5. Optimizing binding affinity with evolutionary strategies\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6e3b54e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6e3b54e",
        "outputId": "60630c14-6883-43a5-91a0-5bbd71bae1b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Collecting esm\n",
            "  Downloading esm-3.0.6-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from esm) (0.19.1+cu121)\n",
            "Collecting torchtext (from esm)\n",
            "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from esm) (7.34.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from esm) (0.8.0)\n",
            "Collecting biotite==0.41.2 (from esm)\n",
            "  Downloading biotite-0.41.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.1 kB)\n",
            "Collecting msgpack-numpy (from esm)\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting biopython (from esm)\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from esm) (1.5.2)\n",
            "Collecting brotli (from esm)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from esm) (24.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from esm) (2.2.2)\n",
            "Requirement already satisfied: cloudpathlib in /usr/local/lib/python3.10/dist-packages (from esm) (0.19.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from esm) (9.0.0)\n",
            "Requirement already satisfied: msgpack>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from biotite==0.41.2->esm) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->esm)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->esm) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->esm) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->esm) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->esm) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->esm) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->esm) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->esm) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->esm) (10.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->esm) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->esm) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->esm) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->esm) (1.16.0)\n",
            "Downloading esm-3.0.6-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biotite-0.41.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.8/35.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, msgpack-numpy, jedi, biopython, biotite, torchtext, esm\n",
            "Successfully installed biopython-1.84 biotite-0.41.2 brotli-1.1.0 esm-3.0.6 jedi-0.19.1 msgpack-numpy-0.4.8 torchtext-0.18.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 1: Setup\n",
        "# Install necessary libraries\n",
        "!pip install transformers torch esm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ee374d",
      "metadata": {
        "id": "34ee374d"
      },
      "source": [
        "\n",
        "### Step 1: Environment Setup\n",
        "We first install the required libraries: `transformers` for working with Hugging Face models, `torch` for PyTorch, and `esm` for protein language modeling tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63aa093c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63aa093c",
        "outputId": "7f487735-eae5-481d-a06c-7a7247ed5d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 2: Load ESM-2 model\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Load the pre-trained ESM-2 model and tokenizer from Hugging Face\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"TianlaiChen/PepMLM-650M\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TianlaiChen/PepMLM-650M\")\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe1a897b",
      "metadata": {
        "id": "fe1a897b"
      },
      "source": [
        "\n",
        "### Step 2: Load ESM-2 Model\n",
        "We use a pre-trained ESM-2 model available on Hugging Face's model hub. ESM-2 is designed for understanding protein sequences, making it suitable for predicting peptide interactions with target proteins.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0031daa6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0031daa6",
        "outputId": "ffc42303-1172-4615-9bf3-6480b61f70ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Protein sequence tokenized.\n",
            "Inputs: {'input_ids': tensor([[ 0, 20,  8,  6, 12,  5,  4,  8, 10,  4,  5, 16,  9, 10, 15,  5, 22, 10,\n",
            "         15, 13, 21, 14, 18,  6, 18,  7,  5,  7, 14, 11, 15, 17, 14, 13,  6, 11,\n",
            "         20, 17,  4, 20, 17, 22,  9, 23,  5, 12, 14,  6, 15, 15,  6, 11, 14, 22,\n",
            "          9,  6,  6,  4, 18, 15,  4, 10, 20,  4, 18, 15, 13, 13, 19, 14,  8,  8,\n",
            "         14, 14, 15, 23, 15, 18,  9, 14, 14,  4, 18, 21, 14, 17,  7, 19, 14,  8,\n",
            "          6, 11,  7, 23,  4,  8, 12,  4,  9,  9, 13, 15, 13, 22, 10, 14,  5, 12,\n",
            "         11, 12, 15, 16, 12,  4,  4,  6, 12, 16,  9,  4,  4, 17,  9, 14, 17, 12,\n",
            "         16, 13, 14,  5, 16,  5,  9,  5, 19, 11, 12, 19, 23, 16, 17, 10,  7,  9,\n",
            "         19,  9, 15, 10,  7, 10,  5, 16,  5, 15, 15, 18,  5, 14,  8,  2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 3: Prepare a protein sequence\n",
        "# Example protein sequence from UniProt (replace with an actual sequence)\n",
        "protein_seq = \"MSGIALSRLAQERKAWRKDHPFGFVAVPTKNPDGTMNLMNWECAIPGKKGTPWEGGLFKLRMLFKDDYPSSPPKCKFEPPLFHPNVYPSGTVCLSILEEDKDWRPAITIKQILLGIQELLNEPNIQDPAQAEAYTIYCQNRVEYEKRVRAQAKKFAPS\"\n",
        "\n",
        "# Tokenize the protein sequence for the model\n",
        "inputs = tokenizer(protein_seq, return_tensors=\"pt\")\n",
        "\n",
        "print(\"Protein sequence tokenized.\")\n",
        "print(\"Inputs:\", inputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4901478",
      "metadata": {
        "id": "e4901478"
      },
      "source": [
        "\n",
        "### Step 3: Generate Peptide Sequence\n",
        "Using PepMLM (Masked Language Modeling), we can generate a peptide sequence that is predicted to bind the input protein. The model predicts the most suitable peptide based on the given protein sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95442008",
      "metadata": {
        "id": "95442008"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_pseudo_perplexity(model, tokenizer, protein_seq, binder_seq):\n",
        "    sequence = protein_seq + binder_seq\n",
        "    original_input = tokenizer.encode(sequence, return_tensors='pt').to(model.device)\n",
        "    length_of_binder = len(binder_seq)\n",
        "\n",
        "    # Prepare a batch with each row having one masked token from the binder sequence\n",
        "    masked_inputs = original_input.repeat(length_of_binder, 1)\n",
        "    positions_to_mask = torch.arange(-length_of_binder - 1, -1, device=model.device)\n",
        "\n",
        "    masked_inputs[torch.arange(length_of_binder), positions_to_mask] = tokenizer.mask_token_id\n",
        "\n",
        "    # Prepare labels for the masked tokens\n",
        "    labels = torch.full_like(masked_inputs, -100)\n",
        "    labels[torch.arange(length_of_binder), positions_to_mask] = original_input[0, positions_to_mask]\n",
        "\n",
        "    # Get model predictions and calculate loss\n",
        "    with torch.no_grad():\n",
        "        outputs = model(masked_inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "    # Loss is already averaged by the model\n",
        "    avg_loss = loss.item()\n",
        "    pseudo_perplexity = np.exp(avg_loss)\n",
        "    return pseudo_perplexity\n",
        "\n",
        "\n",
        "def generate_peptide_for_single_sequence(protein_seq, peptide_length = 15, top_k = 3, num_binders = 4):\n",
        "\n",
        "    peptide_length = int(peptide_length)\n",
        "    top_k = int(top_k)\n",
        "    num_binders = int(num_binders)\n",
        "\n",
        "    binders_with_ppl = []\n",
        "\n",
        "    for _ in range(num_binders):\n",
        "        # Generate binder\n",
        "        masked_peptide = '<mask>' * peptide_length\n",
        "        input_sequence = protein_seq + masked_peptide\n",
        "        inputs = tokenizer(input_sequence, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "        mask_token_indices = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "        logits_at_masks = logits[0, mask_token_indices]\n",
        "\n",
        "        # Apply top-k sampling\n",
        "        top_k_logits, top_k_indices = logits_at_masks.topk(top_k, dim=-1)\n",
        "        probabilities = torch.nn.functional.softmax(top_k_logits, dim=-1)\n",
        "        predicted_indices = Categorical(probabilities).sample()\n",
        "        predicted_token_ids = top_k_indices.gather(-1, predicted_indices.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        generated_binder = tokenizer.decode(predicted_token_ids, skip_special_tokens=True).replace(' ', '')\n",
        "\n",
        "        # Compute PPL for the generated binder\n",
        "        ppl_value = compute_pseudo_perplexity(model, tokenizer, protein_seq, generated_binder)\n",
        "\n",
        "        # Add the generated binder and its PPL to the results list\n",
        "        binders_with_ppl.append([generated_binder, ppl_value])\n",
        "\n",
        "    return binders_with_ppl\n",
        "\n",
        "def generate_peptide(input_seqs, peptide_length=15, top_k=3, num_binders=4):\n",
        "    if isinstance(input_seqs, str):  # Single sequence\n",
        "        binders = generate_peptide_for_single_sequence(input_seqs, peptide_length, top_k, num_binders)\n",
        "        return pd.DataFrame(binders, columns=['Binder', 'Pseudo Perplexity'])\n",
        "\n",
        "    elif isinstance(input_seqs, list):  # List of sequences\n",
        "        results = []\n",
        "        for seq in input_seqs:\n",
        "            binders = generate_peptide_for_single_sequence(seq, peptide_length, top_k, num_binders)\n",
        "            for binder, ppl in binders:\n",
        "                results.append([seq, binder, ppl])\n",
        "        return pd.DataFrame(results, columns=['Input Sequence', 'Binder', 'Pseudo Perplexity'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "321f1ed3",
      "metadata": {
        "id": "321f1ed3"
      },
      "source": [
        "\n",
        "### Step 5: Optimize Peptide Binding Affinity\n",
        "We employ an evolutionary strategy, such as EvoProtGrad, to refine the generated peptide sequence. The goal is to enhance the binding affinity between the peptide and the target protein.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = generate_peptide(protein_seq, peptide_length=15, top_k=3, num_binders=5)\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_UB7KZWuuv6",
        "outputId": "f276ce43-d180-41cf-9bab-ef012b7b390a"
      },
      "id": "W_UB7KZWuuv6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Binder  Pseudo Perplexity\n",
            "0  TDDEPEPLLYAALLE          11.393285\n",
            "1  TQDEPEPLPYLAAEL           9.031120\n",
            "2  FQDSPELLPLYLALL           9.081228\n",
            "3  FDDSEELLLYYLLEL          14.481063\n",
            "4  FEEEEELAPRLRAKL          10.555094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In Silico Directed Evolution of the Peptide Binder with EvoProtGrad and ESM-2"
      ],
      "metadata": {
        "id": "yYf0Hhtbx6SY"
      },
      "id": "yYf0Hhtbx6SY"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evo_prot_grad\n",
        "#del model\n",
        "#torch.cuda.empty_cache()\n",
        "import torch\n",
        "import evo_prot_grad\n",
        "from transformers import AutoTokenizer, EsmForMaskedLM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWAczhCFuzjE",
        "outputId": "7dd9ff0c-5210-4941-ebdd-6b2082c979b0"
      },
      "id": "bWAczhCFuzjE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evo_prot_grad in /usr/local/lib/python3.10/dist-packages (0.2)\n",
            "Requirement already satisfied: transformers==4.38.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.38.0->evo_prot_grad) (4.38.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.38.0->evo_prot_grad) (2.4.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.38.0->evo_prot_grad) (0.34.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]==4.38.0->evo_prot_grad) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.38.0->evo_prot_grad) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.38.0->evo_prot_grad) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.38.0->evo_prot_grad) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0->transformers[torch]==4.38.0->evo_prot_grad) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]==4.38.0->evo_prot_grad) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]==4.38.0->evo_prot_grad) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_evo_prot_grad_on_paired_sequence(paired_protein_sequence):\n",
        "    # Replace ':' with a string of 20 'G' amino acids\n",
        "    separator = 'G' * 20\n",
        "    sequence_with_separator = paired_protein_sequence.replace(':', separator)\n",
        "\n",
        "    # Determine the start and end indices of the first protein and the separator\n",
        "    separator_start_index = sequence_with_separator.find(separator)\n",
        "    first_protein_end_index = separator_start_index\n",
        "    separator_end_index = separator_start_index + len(separator)\n",
        "\n",
        "    # Format the sequence into FASTA format\n",
        "    fasta_format_sequence = f\">Paired_Protein_Sequence\\n{sequence_with_separator}\"\n",
        "\n",
        "    # Save the sequence to a temporary file\n",
        "    temp_fasta_path = \"temp_paired_sequence.fasta\"\n",
        "    with open(temp_fasta_path, \"w\") as file:\n",
        "        file.write(fasta_format_sequence)\n",
        "\n",
        "    # Use a smaller ESM-2 model to reduce memory usage\n",
        "    esm2_expert = evo_prot_grad.get_expert(\n",
        "        'esm',\n",
        "        model=EsmForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\"),  # Use a smaller ESM-2 model\n",
        "        tokenizer=AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\"),\n",
        "        scoring_strategy='pseudolikelihood_ratio',\n",
        "        temperature=0.95,\n",
        "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    )\n",
        "\n",
        "    # Initialize wildtype sequence for the expert with the correct format\n",
        "    wildtype_sequence = sequence_with_separator.replace(\" \", \"\")  # Make sure the input sequence has no spaces\n",
        "    esm2_expert.init_wildtype(wildtype_sequence)\n",
        "\n",
        "    # Initialize Directed Evolution with the preserved first protein and separator region\n",
        "    directed_evolution = evo_prot_grad.DirectedEvolution(\n",
        "        wt_fasta=temp_fasta_path,\n",
        "        output='best',\n",
        "        experts=[esm2_expert],\n",
        "        parallel_chains=1,  # Reduce parallel chains to save memory\n",
        "        n_steps=50,\n",
        "        max_mutations=15,\n",
        "        verbose=True,\n",
        "        preserved_regions=[(0, first_protein_end_index), (separator_start_index, separator_end_index)]\n",
        "    )\n",
        "\n",
        "    # Run the evolution process\n",
        "    variants, scores = directed_evolution()\n",
        "\n",
        "    # Process the results and split them into Protein 1 and Protein 2\n",
        "    for variant, score in zip(variants, scores):\n",
        "        # Remove spaces from the sequence\n",
        "        evolved_sequence_no_spaces = variant.replace(\" \", \"\")\n",
        "\n",
        "        # Split the sequence at the separator\n",
        "        protein_1, protein_2 = evolved_sequence_no_spaces.split(separator)\n",
        "\n",
        "        print(f\"Protein: {protein_1}, Evolved Peptide: {protein_2}, Score: {score}\")"
      ],
      "metadata": {
        "id": "nypB2lq6x_EE"
      },
      "id": "nypB2lq6x_EE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "paired_protein_sequence = \"MSGIALSRLAQERKAWRKDHPFGFVAVPTKNPDGTMNLMNWECAIPGKKGTPWEGGLFKLRMLFKDDYPSSPPKCKFEPPLFHPNVYPSGTVCLSILEEDKDWRPAITIKQILLGIQELLNEPNIQDPAQAEAYTIYCQNRVEYEKRVRAQAKKFAPS:FDEDDPLAPRLLEEE\"  # Replace with your paired protein sequences\n",
        "run_evo_prot_grad_on_paired_sequence(paired_protein_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "RDLIZBQvyO-V",
        "outputId": "f70e3d7d-2a37-4486-9335-000677190037"
      },
      "id": "RDLIZBQvyO-V",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">Wildtype sequence: M S G I A L S R L A Q E R K A W R K D H P F G F V A V P T K N P D G T M N L M N W E C A I P G K K G T P W E G G L F K L R M L F K D D Y P S S P P K C K F E P P L F H P N V Y P S G T V C L S I L E E D K D W R P A I T I K Q I L L G I Q E L L N E P N I Q D P A Q A E A Y T I Y C Q N R V E Y E K R V R A Q A K K F A P S G G G G G G G G G G G G G G G G G G G G F D E D D P L A P R L L E E E\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6c9653ec6ec8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpaired_protein_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"MSGIALSRLAQERKAWRKDHPFGFVAVPTKNPDGTMNLMNWECAIPGKKGTPWEGGLFKLRMLFKDDYPSSPPKCKFEPPLFHPNVYPSGTVCLSILEEDKDWRPAITIKQILLGIQELLNEPNIQDPAQAEAYTIYCQNRVEYEKRVRAQAKKFAPS:FDEDDPLAPRLLEEE\"\u001b[0m  \u001b[0;31m# Replace with your paired protein sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_evo_prot_grad_on_paired_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaired_protein_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-e3d980cbc16d>\u001b[0m in \u001b[0;36mrun_evo_prot_grad_on_paired_sequence\u001b[0;34m(paired_protein_sequence)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Run the evolution process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mvariants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirected_evolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Process the results and split them into Protein 1 and Protein 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evo_prot_grad/common/sampler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;31m# Need to use the string version of the chain to pass to experts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mohs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPoE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_product_of_experts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mgrad_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mohs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPoE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evo_prot_grad/common/sampler.py\u001b[0m in \u001b[0;36m_product_of_experts\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mexpert\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0moh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mohs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexpert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evo_prot_grad/experts/base_experts.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \"\"\"\n\u001b[1;32m    168\u001b[0m         \u001b[0moh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariant_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wt_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evo_prot_grad/common/variant_scoring.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x_oh, x_pred, wt_oh)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwt_score_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_oh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring_strategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pseudolikelihood_ratio\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpseudolikelihood_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring_strategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mutant_marginal\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmutant_marginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwt_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evo_prot_grad/common/variant_scoring.py\u001b[0m in \u001b[0;36mpseudolikelihood_ratio\u001b[0;34m(self, x_oh, logits)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwt_score_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wildtype pseudolikelihood must be set before calling the expert with `init_wildtype`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpseudolikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwt_score_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_oh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e0UGKk6xyXOl"
      },
      "id": "e0UGKk6xyXOl",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}